{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IWSLT English MLM\n",
    "\n",
    "This notebook shows a simple example of how to use the transformer provided by this repo for MLM.\n",
    "\n",
    "We will use the IWSLT 2016 En dataset.\n",
    "\n",
    "This is similar to BERT, except missing some other training tricks, such as NSP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torchtext import data, datasets\n",
    "from torchtext.data import get_tokenizer\n",
    "import spacy\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from model.EncoderDecoder import TransformerEncoder\n",
    "from model.utils import device, Batch, BasicIterator\n",
    "from model.opt import NoamOpt\n",
    "from model.Layers import Linear\n",
    "\n",
    "import time\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The below does some basic data preprocessing and filtering, in addition to setting special tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok = get_tokenizer(\"moses\")\n",
    "PAD = \"<pad>\"\n",
    "SOS = \"<sos>\"\n",
    "EOS = \"<eos>\"\n",
    "\n",
    "en_field = data.Field(tokenize=tok, pad_token=PAD, init_token=SOS, eos_token=EOS)\n",
    "d = data.TabularDataset(\".data/iwslt/de-en/train.en\", format=\"csv\", fields=[(\"text\", en_field)], \n",
    "                    csv_reader_params={\"delimiter\":'\\n'})\n",
    "MIN_FREQ = 4\n",
    "en_field.build_vocab(d.text, min_freq=MIN_FREQ, specials=[\"<mask>\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The batch_size_fn helps deal with dynamic batch size for the torchtext iterator\n",
    "\n",
    "##### The BasicIterator class helps with dynamic batching too, making sure batches are tightly grouped with minimal padding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "global max_text_in_batch\n",
    "def batch_size_fn(new, count, _):\n",
    "    global max_text_in_batch\n",
    "    if count == 1:\n",
    "        max_text_in_batch = 0\n",
    "    max_text_in_batch = max(max_text_in_batch, len(new.text))\n",
    "    return count * max_text_in_batch\n",
    "\n",
    "train_loader = BasicIterator(d, batch_size=1100, device=torch.device(\"cuda\"),\n",
    "                   repeat=False, sort_key=lambda x: (len(x.text)),\n",
    "                   batch_size_fn=batch_size_fn, train=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Single step over entire dataset, with tons of gradient accumulation to get batch sizes big enough for stable training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(dataloader):\n",
    "    i = 0\n",
    "    loss = 0\n",
    "    total_loss = 0\n",
    "    for batch in dataloader:\n",
    "        # Only take a step every 20th batch\n",
    "        if (i + 1) % 20 == 0:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        loss, _, _ = transformer.mask_forward_and_return_loss(criterion, batch.text, .15)\n",
    "        loss.backward()\n",
    "        total_loss += loss.item()\n",
    "        i += 1\n",
    "\n",
    "    return total_loss / i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating the pseudoBERT:\n",
    "\n",
    "Subclassing the TransformerEncoder class allows us to implement a forward_and_return_loss_function easily, and requires nothing else before being fully functional. \n",
    "\n",
    "The TransformerEncoder class handles embedding and the transformer encoder layers itself, we simply need to follow it up with a single Linear layer. The masking is a bit complex, but should be understandable with the below comments.\n",
    "\n",
    "The goal of MLM is to randomly mask tokens, then train a model to predict what the ground truth token actually is. This is a hard task that requires good understanding of language itself.\n",
    "\n",
    "We use the utils.Batch object to automatically create padding masks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLM(TransformerEncoder):\n",
    "    def __init__(self, input_vocab_size,embedding_dim,\n",
    "        n_layers,hidden_dim,n_heads,dropout_rate,\n",
    "        pad_idx,mask_idx,):\n",
    "        \n",
    "        super(MLM, self).__init__(input_vocab_size,embedding_dim, n_layers,\n",
    "                                  hidden_dim,n_heads,dropout_rate,pad_idx,)\n",
    "        \n",
    "        self.pad_idx = pad_idx\n",
    "        self.mask_idx = mask_idx\n",
    "        \n",
    "        self.fc1 = Linear(embedding_dim, input_vocab_size)\n",
    "        \n",
    "    def mask_forward_and_return_loss(self, criterion, seq, mask_rate):\n",
    "        \"\"\"\n",
    "        Pass input through transformer encoder and returns loss, handles masking for\n",
    "        both MLM and padding automagically\n",
    "        Args:\n",
    "            criterion: torch.nn.functional loss function of choice\n",
    "            sources: source sequences, [seq_len, bs]\n",
    "            mask_rate: masking rate for non-padding tokens\n",
    "\n",
    "        Returns:\n",
    "            loss, transformer output, mask\n",
    "        \"\"\"\n",
    "        # count number of tokens that are padding\n",
    "        number_of_pad_tokens = torch.sum(\n",
    "            torch.where(seq == self.pad_idx, torch.ones_like(seq),\n",
    "                        torch.zeros_like(seq)\n",
    "                       ).float())\n",
    "        # Don't mask pad tokens, scale mask ratio up accordingly\n",
    "        num_tokens = np.prod(seq.size())\n",
    "        # clamp to prevent errors if there are a huge amount of padding\n",
    "        # tokens in a given batch (> 70%)\n",
    "        true_masking_rate = torch.clamp((1 / (1 - (number_of_pad_tokens / num_tokens))) * mask_rate, 0, 1)\n",
    "        bernoulli_probabilities = torch.zeros_like(seq) + true_masking_rate\n",
    "        masking_mask = torch.bernoulli(bernoulli_probabilities).long().to(device)\n",
    "        masked_seq = torch.where(torch.logical_and((seq != self.pad_idx), (masking_mask == 1)), \n",
    "                                                   (torch.ones_like(seq) * self.mask_idx).to(device), seq) \n",
    "        \n",
    "        batch = Batch(masked_seq, None, self.pad_idx)\n",
    "        out = self.forward(batch.src.to(device), batch.src_mask.to(device))\n",
    "        out = self.fc1(out.transpose(0, 1)).transpose(0, 1)\n",
    "        # zeroing out token predictions on non-masked tokens\n",
    "        out = out * masking_mask.unsqueeze(-1)\n",
    "        \n",
    "                \n",
    "        loss = criterion(\n",
    "            out.contiguous().view(-1, out.size(-1)),\n",
    "            # ((A-1) @ M) + 1 = A is 1 where B is 0, and otherwise unchanged\n",
    "            # This makes loss only depend on masked tokens, like BERT\n",
    "            (((seq-1) * masking_mask) + 1).contiguous().view(-1),\n",
    "            ignore_index=self.pad_idx,\n",
    "        )\n",
    "        \n",
    "        return loss, out, masking_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Here we instantiate the model and set hyperparameters. Note: this MLM model is extremely small for ease of recreating experiments. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_vocab_size = len(en_field.vocab)\n",
    "embedding_dim = 512\n",
    "n_layers = 4\n",
    "hidden_dim = 1024\n",
    "n_heads = 4\n",
    "dropout_rate = .1\n",
    "pad_idx = 1\n",
    "mask_idx = 4\n",
    "transformer = MLM(input_vocab_size, embedding_dim, n_layers, hidden_dim,\n",
    "           n_heads, dropout_rate, pad_idx, mask_idx).to(device)\n",
    "\n",
    "adamopt = torch.optim.Adam(transformer.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9)\n",
    "optimizer = NoamOpt(embedding_dim, 1, 2000, adamopt)\n",
    "criterion = F.cross_entropy\n",
    "\n",
    "# optimization is unstable without this step\n",
    "for p in transformer.parameters():\n",
    "        if p.dim() > 1:\n",
    "            nn.init.xavier_uniform_(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's run 5 epochs over the entire dataset, printing loss once per epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1. Loss: 6.848, \n",
      "Total time (s): 373, Last epoch time (s): 373\n",
      "Epoch 2. Loss: 5.246, \n",
      "Total time (s): 746, Last epoch time (s): 373\n",
      "Epoch 3. Loss: 4.379, \n",
      "Total time (s): 1123, Last epoch time (s): 377\n",
      "Epoch 4. Loss: 4.067, \n",
      "Total time (s): 1499, Last epoch time (s): 376\n",
      "Epoch 5. Loss: 3.876, \n",
      "Total time (s): 1877, Last epoch time (s): 377\n"
     ]
    }
   ],
   "source": [
    "true_start = time.time()\n",
    "for i in range(5):\n",
    "    transformer.train()\n",
    "    t = time.time()\n",
    "    \n",
    "    loss = train_step(train_loader)\n",
    "    print(\"Epoch {}. Loss: {}, \".format((i+1), str(loss)[:5], int(time.time() - t)))\n",
    "    print(\"Total time (s): {}, Last epoch time (s): {}\".format(int(time.time()- true_start), int(time.time() - t)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\conda_reinstall\\condainstall\\envs\\torch\\lib\\site-packages\\torch\\serialization.py:402: UserWarning: Couldn't retrieve source code for container of type MLM. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "torch.save(transformer, \"basic_MLM.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Let's go ahead and process some random example sentences not in the training data, and vizualize the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer.eval()\n",
    "inp = en_field.process([tok(\"Now, if we all played football, this wouldn't be an issue.\"), \n",
    "                        tok(\"I don't really agree with you, honestly\"),\n",
    "                       tok(\"Not all who wander are lost.\")]).to(device)\n",
    "_, pred, mask = transformer.mask_forward_and_return_loss(criterion, inp, .20)\n",
    "pred = pred.transpose(0, 1)\n",
    "mask = mask.transpose(0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Simple code for visualization. Let's check out how our model did."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_model_predictions(inp, pred, mask):\n",
    "    print(\"Sentence:\", end=\" \")\n",
    "    for i in range(len(inp)):\n",
    "        if en_field.vocab.itos[inp[i]] == \"<eos>\":\n",
    "            break\n",
    "        if mask[i] == 1:\n",
    "            print(\"<\" + en_field.vocab.itos[pred[i]] +  \" | \" + en_field.vocab.itos[inp[i]] + \">\", end = \" \")\n",
    "        else:\n",
    "            print(en_field.vocab.itos[inp[i]], end = \" \")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Masked tokens are surrounded by < >. The word on the left is the prediction, the word on the right is the ground truth. They're seperated by a |.\n",
    "\n",
    "##### Despite being a small model, the predictions are fairly accurate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: <sos> Now <, | ,> if we all played football <, | ,> this wouldn <&apos;t | &apos;t> be an issue . \n",
      "\n",
      "Sentence: <sos> I <don | don> &apos;t <just | really> agree with you <. | ,> honestly \n",
      "\n",
      "Sentence: <sos> <And | Not> all who wander are <there | lost> . \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(inp.T)):\n",
    "    visualize_model_predictions(\n",
    "        inp.transpose(0, 1)[i].tolist(), \n",
    "        torch.argmax(pred[i], dim=-1).tolist(), \n",
    "        mask[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
