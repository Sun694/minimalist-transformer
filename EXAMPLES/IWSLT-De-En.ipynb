{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IWSLT German->English translation\n",
    "\n",
    "This notebook shows a simple example of how to use the transformer provided by this repo for one-direction translation. \n",
    "\n",
    "We will use the IWSLT 2016 De-En dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext import data, datasets\n",
    "import spacy\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from model.transformers import BaseTransformer\n",
    "\n",
    "from model.utils import device, Batch, BasicIterator\n",
    "from model.opt import NoamOpt\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The below does some basic data preprocessing and filtering, in addition to setting special tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "de_data = spacy.load('de_core_news_sm')\n",
    "en_data = spacy.load('en_core_web_sm')\n",
    "\n",
    "def de_tokenizer(data):\n",
    "    raw_data = [x.text for x in de_data.tokenizer(data)]\n",
    "    return raw_data\n",
    "def en_tokenizer(data):\n",
    "    raw_data = [x.text for x in en_data.tokenizer(data)]\n",
    "    return raw_data\n",
    "\n",
    "\n",
    "BOS = \"<s>\"\n",
    "EOS = \"</s>\"\n",
    "BLANK = \"<blank>\"\n",
    "\n",
    "de = data.Field(tokenize=de_tokenizer, pad_token=BLANK, init_token=BOS, eos_token=EOS)\n",
    "en = data.Field(tokenize=en_tokenizer, pad_token=BLANK, init_token=BOS, eos_token=EOS)\n",
    "\n",
    "MAX_LEN = 128\n",
    "\n",
    "train, val, test = datasets.IWSLT.splits(\n",
    "    exts=(\".de\", \".en\"), fields=(de, en),\n",
    "    filter_pred=lambda x: len(vars(x)['src']) <= MAX_LEN and len(vars(x)['trg']) <= MAX_LEN\n",
    ")\n",
    "\n",
    "MIN_FREQ = 4\n",
    "\n",
    "de.build_vocab(train.src, min_freq=MIN_FREQ)\n",
    "en.build_vocab(train.trg, min_freq=MIN_FREQ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Torchtext required functions. batch_size_fn exists to make sure the batch size stays where it should be.\n",
    "\n",
    "##### The BasicIterator class helps with dynamic batching, so batches are tightly grouped with minimal padding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "global max_src_in_batch, max_tgt_in_batch\n",
    "def batch_size_fn(new, count, sofar):\n",
    "    \"Keep augmenting batch and calculate total number of tokens + padding.\"\n",
    "    global max_src_in_batch, max_tgt_in_batch\n",
    "    if count == 1:\n",
    "        max_src_in_batch = 0\n",
    "        max_tgt_in_batch = 0\n",
    "    max_src_in_batch = max(max_src_in_batch,  len(new.src))\n",
    "    max_tgt_in_batch = max(max_tgt_in_batch,  len(new.trg) + 2)\n",
    "    src_elements = count * max_src_in_batch\n",
    "    tgt_elements = count * max_tgt_in_batch\n",
    "    return max(src_elements, tgt_elements)\n",
    "\n",
    "train_loader = BasicIterator(train, batch_size=1100, device=torch.device(\"cuda\"),\n",
    "                   repeat=False, sort_key=lambda x: (len(x.src), len(x.trg)),\n",
    "                   batch_size_fn=batch_size_fn, train=True)\n",
    "val_loader = BasicIterator(val, batch_size=1100, device=torch.device(\"cuda\"),\n",
    "                   repeat=False, sort_key=lambda x: (len(x.src), len(x.trg)),\n",
    "                   batch_size_fn=batch_size_fn, train=False)\n",
    "test_loader = BasicIterator(test, batch_size=1100, device=torch.device(\"cuda\"),\n",
    "                   repeat=False, sort_key=lambda x: (len(x.src), len(x.trg)),\n",
    "                   batch_size_fn=batch_size_fn, train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Single step over entire dataset, with tons of gradient accumulation to get batch sizes big enough for stable training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(dataloader):\n",
    "    i = 0\n",
    "    loss = 0\n",
    "    total_loss = 0\n",
    "    for batch in dataloader:\n",
    "        source = batch.src\n",
    "        target = batch.trg\n",
    "        # Only take a step every 11th batch to simulate bs of ~12k\n",
    "        if (i + 1) % 11 == 0:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        loss, _ = transformer.forward_and_return_loss(criterion, source, target)\n",
    "        loss.backward()\n",
    "        total_loss += loss.item()\n",
    "        i += 1\n",
    "\n",
    "    return total_loss / i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating the translation model:\n",
    "\n",
    "Subclassing the Transformer class allows us to implement a forward_and_return_loss_function and generation function, and requires nothing else before being fully functional. \n",
    "\n",
    "The Transformer class handles embedding and the transformer layers itself, including an output Linear layer.\n",
    "\n",
    "The goal of a basic translation model is to recreate the translation given the input (in a different language). We use crossentropy between the target and ground truth.\n",
    "\n",
    "We use the utils.Batch object to automatically create padding masks, in addition to dec-dec attn. masks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TranslationModel(BaseTransformer):\n",
    "    def __init__(\n",
    "        self, *args,\n",
    "    ):\n",
    "        super(TranslationModel, self).__init__(*args)\n",
    "\n",
    "    def forward_and_return_loss(self, criterion, sources, targets):\n",
    "        \"\"\"\n",
    "        Pass input through transformer and return loss, handles masking automagically\n",
    "        Args:\n",
    "            criterion: torch.nn.functional loss function of choice\n",
    "            sources: source sequences, [seq_len, bs]\n",
    "            targets: full target sequence, [seq_len, bs, embedding_dim]\n",
    "\n",
    "        Returns:\n",
    "            loss, transformer output\n",
    "        \"\"\"\n",
    "\n",
    "        batch = Batch(sources, targets, self.pad_idx)\n",
    "        seq_len, batch_size = batch.trg.size()\n",
    "        out = self.forward(batch.src, batch.trg, batch.src_mask, batch.trg_mask)\n",
    "        loss = criterion(\n",
    "            out.contiguous().view(-1, out.size(-1)),\n",
    "            batch.trg_y.contiguous().view(-1),\n",
    "            ignore_index=self.pad_idx,\n",
    "        )\n",
    "\n",
    "        return loss, out\n",
    "\n",
    "    def generate(self, source, source_mask, max_len):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            source: input sequence indices, [seq_len, bs,\n",
    "            source_mask: the source mask to prevent attending to <pad> tokens\n",
    "            max_len: maximum length\n",
    "\n",
    "        Returns:\n",
    "            generated translations\n",
    "        \"\"\"\n",
    "        memory = self.encoder(source, source_mask)\n",
    "        ys = torch.ones(1, source.size(1)).long().fill_(self.sos_idx).to(device)\n",
    "        # max target length is 1.5x * source + 10 to save compute power\n",
    "        for _ in range(int(1.5 * source.size(0)) - 1 + 10):\n",
    "            out = self.decoder(ys, memory, source_mask, Batch(ys, ys, 1).raw_mask)\n",
    "            out = self.fc1(out[-1].unsqueeze(0))\n",
    "            prob = F.log_softmax(out, dim=-1)\n",
    "            next_word = torch.argmax(prob, dim=-1)\n",
    "            ys = torch.cat([ys, next_word.detach()], dim=0)\n",
    "\n",
    "        return ys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### These hyperparameters were set for a GTX980. A bigger GPU, such as a P100 or similar, will be able to handle default transformer hyperparameters and bigger batch sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "input_vocab_size = len(de.vocab)\n",
    "output_vocab_size = len(en.vocab)\n",
    "embedding_dim = 256\n",
    "n_layers = 4\n",
    "hidden_dim = 512\n",
    "n_heads = 8\n",
    "dropout_rate = .1\n",
    "transformer = TranslationModel(input_vocab_size, output_vocab_size, embedding_dim, \n",
    "                               n_layers,hidden_dim, n_heads, dropout_rate).to(device)\n",
    "\n",
    "adamopt = torch.optim.Adam(transformer.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9)\n",
    "optimizer = NoamOpt(embedding_dim, 1, 2000, adamopt)\n",
    "criterion = F.cross_entropy\n",
    "\n",
    "# optimization is unstable without this step\n",
    "for p in transformer.parameters():\n",
    "        if p.dim() > 1:\n",
    "            nn.init.xavier_uniform_(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Runs 10 epochs of the entire training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1. Loss: 6.035, \n",
      "Total time: 439, Last epoch time (s): 439\n",
      "Epoch 2. Loss: 4.087, \n",
      "Total time: 901, Last epoch time (s): 461\n",
      "Epoch 3. Loss: 3.289, \n",
      "Total time: 1346, Last epoch time (s): 445\n",
      "Epoch 4. Loss: 2.814, \n",
      "Total time: 1786, Last epoch time (s): 439\n",
      "Epoch 5. Loss: 2.519, \n",
      "Total time: 2247, Last epoch time (s): 461\n",
      "Epoch 6. Loss: 2.295, \n",
      "Total time: 2674, Last epoch time (s): 426\n",
      "Epoch 7. Loss: 2.053, \n",
      "Total time: 3102, Last epoch time (s): 427\n",
      "Epoch 8. Loss: 1.858, \n",
      "Total time: 3527, Last epoch time (s): 425\n",
      "Epoch 9. Loss: 1.698, \n",
      "Total time: 3951, Last epoch time (s): 424\n",
      "Epoch 10. Loss: 1.561, \n",
      "Total time: 4376, Last epoch time (s): 424\n"
     ]
    }
   ],
   "source": [
    "true_start = time.time()\n",
    "for i in range(10):\n",
    "    transformer.train()\n",
    "    t = time.time()\n",
    "    \n",
    "    loss = train_step(train_loader)\n",
    "    \n",
    "    print(\"Epoch {}. Loss: {}, \".format((i+1), str(loss)[:5], int(time.time() - t)))\n",
    "    print(\"Total time (s): {}, Last epoch time (s): {}\".format(int(time.time()- true_start), int(time.time() - t)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(transformer, \"basic_translation.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Finally, generations. \n",
    "\n",
    "\n",
    "The model by default uses greedy decoding for generation, and does not have incremental decoding. Currently, this leads to the transformer generating at about 1/2 the speed of Fairseq for short sequences. \n",
    "\n",
    "Implementing incremental decoding, however, makes the code for the attention function much harder to read, and has been left out for now. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sentence: Und der Garten ist wunderschön . \n",
      "Predicted translation: And the garden is beautiful . \n",
      "Ground truth translation: And the garden , it was beautiful . \n",
      "\n",
      "Input sentence: Die <unk> ist nicht nachhaltig . \n",
      "Predicted translation: The <unk> system is n't sustainable . \n",
      "Ground truth translation: The internal combustion engine is not sustainable . \n",
      "\n",
      "Input sentence: Wir sehen immer dieselben Symptome . \n",
      "Predicted translation: We see the same symptoms . \n",
      "Ground truth translation: We see all the same symptoms . \n",
      "\n",
      "Input sentence: Sie ist keine <unk> . \" \n",
      "Predicted translation: It 's not a <unk> . \" \n",
      "Ground truth translation: She 's not North Korean . \" \n",
      "\n",
      "Input sentence: Weil es so schön klingt . \n",
      "Predicted translation: Because it sounds beautiful . \n",
      "Ground truth translation: Just because it sounds so good . \n",
      "\n",
      "Input sentence: Aber man muss es pflegen . \n",
      "Predicted translation: But you have to care about it . \n",
      "Ground truth translation: But you have to maintain it . \n",
      "\n",
      "Input sentence: Das kann man alles sehen . \n",
      "Predicted translation: You can see it all . \n",
      "Ground truth translation: You can see all of this . \n",
      "\n",
      "Input sentence: Es geht um uns alle . \n",
      "Predicted translation: It 's all about us . \n",
      "Ground truth translation: It 's about all of us . \n",
      "\n"
     ]
    }
   ],
   "source": [
    "transformer.eval()\n",
    "new_batch = next(iter(val_loader))\n",
    "inp = new_batch.src\n",
    "tra = new_batch.trg\n",
    "\n",
    "out = transformer.generate(inp, Batch(inp, inp, 1).src_mask, 120)\n",
    "for i in range(len(inp)):\n",
    "    print(\"Input sentence: \", end=\"\")\n",
    "    for j in range(1, inp.size(0)):\n",
    "        char = de.vocab.itos[inp[j, i]]\n",
    "        if char == \"</s>\": \n",
    "            break\n",
    "        print(char, end =\" \")\n",
    "    print(\"\\nPredicted translation: \", end=\"\")\n",
    "    for j in range(1, out.size(0)):\n",
    "        char = en.vocab.itos[out[j, i]]\n",
    "        if char == \"</s>\": \n",
    "            break\n",
    "        print(char, end =\" \")\n",
    "    print(\"\\nGround truth translation: \", end=\"\")\n",
    "    for j in range(1, tra.size(0)):\n",
    "        char = en.vocab.itos[tra[j, i]]\n",
    "        if char == \"</s>\": \n",
    "            break\n",
    "        print(char, end =\" \")    \n",
    "    print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
