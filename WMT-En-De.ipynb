{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext import data, datasets\n",
    "import spacy\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "\n",
    "from model.transformers import MLT\n",
    "\n",
    "from model.utils import device, Batch\n",
    "from model.opt import NoamOpt\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "de_data = spacy.load('de_core_news_sm')\n",
    "en_data = spacy.load('en_core_web_sm')\n",
    "\n",
    "def de_tokenizer(data):\n",
    "    raw_data = [x.text for x in de_data.tokenizer(data)]\n",
    "    return raw_data\n",
    "def en_tokenizer(data):\n",
    "    raw_data = [x.text for x in en_data.tokenizer(data)]\n",
    "    return raw_data\n",
    "\n",
    "\n",
    "BOS = \"<s>\"\n",
    "EOS = \"</s>\"\n",
    "BLANK = \"<blank>\"\n",
    "\n",
    "de = data.Field(tokenize=de_tokenizer, pad_token=BLANK, init_token=BOS, eos_token=EOS)\n",
    "en = data.Field(tokenize=en_tokenizer, pad_token=BLANK, init_token=BOS, eos_token=EOS)\n",
    "\n",
    "MAX_LEN = 128\n",
    "\n",
    "train, val, test = datasets.IWSLT.splits(\n",
    "    exts=(\".de\", \".en\"), fields=(de, en),\n",
    "    filter_pred=lambda x: len(vars(x)['src']) <= MAX_LEN and len(vars(x)['trg']) <= MAX_LEN\n",
    ")\n",
    "\n",
    "MIN_FREQ = 4\n",
    "\n",
    "de.build_vocab(train.src, min_freq=MIN_FREQ)\n",
    "en.build_vocab(train.trg, min_freq=MIN_FREQ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# This class, inspired by \"The Annotated Transformer\", searches\n",
    "# over tons of batches to find tight clusters of sentence\n",
    "# lengths. This is to keep padding minimal.\n",
    "class BasicIterator(data.Iterator):\n",
    "    def create_batches(self):\n",
    "        if self.train:\n",
    "            def pool_batch(d, random_shuffler):\n",
    "                for p in data.batch(d, self.batch_size * 100):\n",
    "                    p_batch = data.batch(\n",
    "                        sorted(p, key=self.sort_key),\n",
    "                        self.batch_size, self.batch_size_fn\n",
    "                    )\n",
    "                    for b in random_shuffler(list(p_batch)):\n",
    "                        yield b\n",
    "            self.batches = pool_batch(self.data(), self.random_shuffler)\n",
    "        else:\n",
    "            self.batches = []\n",
    "            for b in data.batch(self.data(), self.batch_size, self.batch_size_fn):\n",
    "                self.batches.append(sorted(b, key=self.sort_key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "global max_src_in_batch, max_tgt_in_batch\n",
    "def batch_size_fn(new, count, sofar):\n",
    "    \"Keep augmenting batch and calculate total number of tokens + padding.\"\n",
    "    global max_src_in_batch, max_tgt_in_batch\n",
    "    if count == 1:\n",
    "        max_src_in_batch = 0\n",
    "        max_tgt_in_batch = 0\n",
    "    max_src_in_batch = max(max_src_in_batch,  len(new.src))\n",
    "    max_tgt_in_batch = max(max_tgt_in_batch,  len(new.trg) + 2)\n",
    "    src_elements = count * max_src_in_batch\n",
    "    tgt_elements = count * max_tgt_in_batch\n",
    "    return max(src_elements, tgt_elements)\n",
    "\n",
    "train_loader = BasicIterator(train, batch_size=1100, device=torch.device(\"cuda\"),\n",
    "                   repeat=False, sort_key=lambda x: (len(x.src), len(x.trg)),\n",
    "                   batch_size_fn=batch_size_fn, train=True)\n",
    "val_loader = BasicIterator(val, batch_size=1100, device=torch.device(\"cuda\"),\n",
    "                   repeat=False, sort_key=lambda x: (len(x.src), len(x.trg)),\n",
    "                   batch_size_fn=batch_size_fn, train=False)\n",
    "test_loader = BasicIterator(test, batch_size=1100, device=torch.device(\"cuda\"),\n",
    "                   repeat=False, sort_key=lambda x: (len(x.src), len(x.trg)),\n",
    "                   batch_size_fn=batch_size_fn, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(dataloader):\n",
    "    i = 0\n",
    "    loss = 0\n",
    "    total_loss = 0\n",
    "    for batch in dataloader:\n",
    "        source = batch.src\n",
    "        target = batch.trg\n",
    "        # Only take a step every 11th batch to simulate bs of ~12k\n",
    "        if (i + 1) % 11 == 0:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        loss, _ = transformer.forward_and_return_loss(criterion, source, target)\n",
    "        loss.backward()\n",
    "        total_loss += loss.item()\n",
    "        i += 1\n",
    "\n",
    "    return total_loss / i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "embedding_dim = 256\n",
    "n_layers = 4\n",
    "hidden_dim = 512\n",
    "n_heads = 8\n",
    "dropout_rate = .1\n",
    "transformer = MLT(len(de.vocab), len(en.vocab), embedding_dim, n_layers,\n",
    "                   hidden_dim, n_heads, dropout_rate).to(device)\n",
    "\n",
    "adamopt = torch.optim.Adam(transformer.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9)\n",
    "optimizer = NoamOpt(embedding_dim, 1, 2000, adamopt)\n",
    "criterion = F.cross_entropy\n",
    "\n",
    "# optimization is unstable without this step\n",
    "for p in transformer.parameters():\n",
    "        if p.dim() > 1:\n",
    "            nn.init.xavier_uniform_(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1. Loss: 6.035, \n",
      "Total time: 439, Last epoch time (s): 439\n",
      "Epoch 2. Loss: 4.087, \n",
      "Total time: 901, Last epoch time (s): 461\n",
      "Epoch 3. Loss: 3.289, \n",
      "Total time: 1346, Last epoch time (s): 445\n",
      "Epoch 4. Loss: 2.814, \n",
      "Total time: 1786, Last epoch time (s): 439\n",
      "Epoch 5. Loss: 2.519, \n",
      "Total time: 2247, Last epoch time (s): 461\n",
      "Epoch 6. Loss: 2.295, \n",
      "Total time: 2674, Last epoch time (s): 426\n",
      "Epoch 7. Loss: 2.053, \n",
      "Total time: 3102, Last epoch time (s): 427\n",
      "Epoch 8. Loss: 1.858, \n",
      "Total time: 3527, Last epoch time (s): 425\n",
      "Epoch 9. Loss: 1.698, \n",
      "Total time: 3951, Last epoch time (s): 424\n",
      "Epoch 10. Loss: 1.561, \n",
      "Total time: 4376, Last epoch time (s): 424\n"
     ]
    }
   ],
   "source": [
    "true_start = time.time()\n",
    "for i in range(10):\n",
    "    transformer.train()\n",
    "    t = time.time()\n",
    "    \n",
    "    loss = train_step(train_loader)\n",
    "    \n",
    "    print(\"Epoch {}. Loss: {}, \".format((i+1), str(loss)[:5], int(time.time() - t)))\n",
    "    print(\"Total time: {}, Last epoch time (s): {}\".format(int(time.time()- true_start), int(time.time() - t)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(transformer, \"model_save.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sentence: Und der Garten ist wunderschön . \n",
      "Predicted translation: And the garden is beautiful . \n",
      "Ground truth translation: And the garden , it was beautiful . \n",
      "\n",
      "Input sentence: Die <unk> ist nicht nachhaltig . \n",
      "Predicted translation: The <unk> system is n't sustainable . \n",
      "Ground truth translation: The internal combustion engine is not sustainable . \n",
      "\n",
      "Input sentence: Wir sehen immer dieselben Symptome . \n",
      "Predicted translation: We see the same symptoms . \n",
      "Ground truth translation: We see all the same symptoms . \n",
      "\n",
      "Input sentence: Sie ist keine <unk> . \" \n",
      "Predicted translation: It 's not a <unk> . \" \n",
      "Ground truth translation: She 's not North Korean . \" \n",
      "\n",
      "Input sentence: Weil es so schön klingt . \n",
      "Predicted translation: Because it sounds beautiful . \n",
      "Ground truth translation: Just because it sounds so good . \n",
      "\n",
      "Input sentence: Aber man muss es pflegen . \n",
      "Predicted translation: But you have to care about it . \n",
      "Ground truth translation: But you have to maintain it . \n",
      "\n",
      "Input sentence: Das kann man alles sehen . \n",
      "Predicted translation: You can see it all . \n",
      "Ground truth translation: You can see all of this . \n",
      "\n",
      "Input sentence: Es geht um uns alle . \n",
      "Predicted translation: It 's all about us . \n",
      "Ground truth translation: It 's about all of us . \n",
      "\n"
     ]
    }
   ],
   "source": [
    "transformer.eval()\n",
    "new_batch = next(iter(val_loader))\n",
    "inp = new_batch.src\n",
    "tra = new_batch.trg\n",
    "\n",
    "out = transformer.generate(inp, Batch(inp, inp, 1).src_mask, 120)\n",
    "for i in range(len(inp)):\n",
    "    print(\"Input sentence: \", end=\"\")\n",
    "    for j in range(1, inp.size(0)):\n",
    "        char = de.vocab.itos[inp[j, i]]\n",
    "        if char == \"</s>\": \n",
    "            break\n",
    "        print(char, end =\" \")\n",
    "    print(\"\\nPredicted translation: \", end=\"\")\n",
    "    for j in range(1, out.size(0)):\n",
    "        char = en.vocab.itos[out[j, i]]\n",
    "        if char == \"</s>\": \n",
    "            break\n",
    "        print(char, end =\" \")\n",
    "    print(\"\\nGround truth translation: \", end=\"\")\n",
    "    for j in range(1, tra.size(0)):\n",
    "        char = en.vocab.itos[tra[j, i]]\n",
    "        if char == \"</s>\": \n",
    "            break\n",
    "        print(char, end =\" \")    \n",
    "    print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}